{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLUE SQUARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./archive/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(img): \n",
    "\n",
    "    # remove noise\n",
    "    # Using a Median Filter\n",
    "    img_smoothed = cv2.medianBlur(img, 5)\n",
    "\n",
    "    return img_smoothed\n",
    "    # return img\n",
    "\n",
    "def contrastAdjust(img):\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    h, s, v = img[:, :, 0], img[:, :, 1], img[:, :, 2]\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "\n",
    "    v = clahe.apply(v)\n",
    "\n",
    "    img = np.dstack((h, s, v))\n",
    "\n",
    "    img_clean = cv2.cvtColor(img, cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    return img_clean\n",
    "\n",
    "def removeShadows(img):\n",
    "    rgb_planes = cv2.split(img)\n",
    "    result_norm_planes = []\n",
    "    for plane in rgb_planes:\n",
    "        dilated_img = cv2.dilate(plane, np.ones((7, 7), np.uint8))\n",
    "        bg_img = cv2.medianBlur(dilated_img, 41)\n",
    "        diff_img = 255 - cv2.absdiff(plane, bg_img)\n",
    "        norm_img = cv2.normalize(\n",
    "            diff_img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
    "        result_norm_planes.append(norm_img)\n",
    "\n",
    "    img_clean = cv2.merge(result_norm_planes)\n",
    "\n",
    "    return img_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CHANGE THIS\n",
    "\n",
    "def calcAngles(cnt_img, corners):\n",
    "    angles = []\n",
    "    \n",
    "    # Calculate euclidean distance between each corner\n",
    "    dist1 = int(math.sqrt(math.pow(corners[0] - corners[2], 2) +\n",
    "                       math.pow(corners[1] - corners[3], 2)))\n",
    "    dist2 = int(math.sqrt(math.pow(corners[2] - corners[4], 2) +\n",
    "                       math.pow(corners[3] - corners[5], 2)))\n",
    "    dist3 = int(math.sqrt(math.pow(corners[4] - corners[6], 2) +\n",
    "                   math.pow(corners[5] - corners[7], 2)))\n",
    "    dist4 = int(math.sqrt(math.pow(corners[6] - corners[0], 2) +\n",
    "                   math.pow(corners[7] - corners[1], 2)))\n",
    "\n",
    "    \n",
    "    max_radius = min([dist1, dist2, dist3, dist4])\n",
    "\n",
    "    for i in range(len(corners[:-1]), 2):\n",
    "        blank_img = np.zeros((len(cnt_img), len(cnt_img[0])), np.uint8)\n",
    "        cv2.circle(blank_img, (corners[i], corners[i + 1]),\n",
    "                  max_radius // 2, (255, 255, 255))\n",
    "        intersect_img = cv2.bitwise_and(cnt_img, blank_img)\n",
    "\n",
    "        intersect_pts = np.where(intersect_img > 1)\n",
    "\n",
    "        if(len(intersect_pts[0]) < 2 or len(intersect_pts[1]) < 2):\n",
    "            angles.append(0)\n",
    "            continue\n",
    "\n",
    "        vector1 = (intersect_pts[1][0] - corners[i],\n",
    "                   intersect_pts[0][0] - corners[i + 1])\n",
    "        vector2 = (intersect_pts[1][1] - corners[i],\n",
    "                   intersect_pts[0][1] - corners[i + 1])\n",
    "\n",
    "        scalar_p = vector1[0] * vector2[0] + vector1[1] * vector2[1]\n",
    "        norm1 = math.sqrt(math.pow(vector1[0], 2) + math.pow(vector1[1], 2))\n",
    "        norm2 = math.sqrt(math.pow(vector2[0], 2) + math.pow(vector2[1], 2))\n",
    "\n",
    "        angle = math.acos(scalar_p / (norm1 * norm2)) * 180 / math.pi\n",
    "        angles.append(angle)\n",
    "\n",
    "    return angles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureRecognition(img): \n",
    "\n",
    "    # Segmentation to recognize colour blue\n",
    "    # Converts images from BGR to HSV\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Bounds adjusted to identify shades of blue correspondent to traffic signs\n",
    "    lower_blue = np.array([100, 100, 80])\n",
    "    upper_blue = np.array([150, 255, 255])\n",
    "\n",
    "    # Here we are defining range of bluecolor in HSV\n",
    "    # This creates a mask of blue coloured\n",
    "    # objects found in the frame.\n",
    "    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "    # The bitwise and of the frame and mask is done so\n",
    "    # that only the blue coloured objects are highlighted\n",
    "    # and stored in res\n",
    "    res = cv2.bitwise_and(img, img, mask=mask)\n",
    "    cv2.imshow('original', img)\n",
    "    cv2.imshow('mask', mask)\n",
    "    cv2.imshow('res', res)\n",
    "\n",
    "\n",
    "\n",
    "    # TEST SEGMENTATION\n",
    "    cv2.waitKey()\n",
    "\n",
    "    # Destroys all of the HighGUI windows.\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    # Post-Segmentation smoothing \n",
    "    # res = smooth(res)  \n",
    "\n",
    "\n",
    "    # Contours on Resulting Image\n",
    "    gray_img = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Canny\n",
    "    canny = cv2.Canny(gray_img, 130, 255)\n",
    "\n",
    "    # Find Contours\n",
    "    # RETR_CCOMP: retrieves all of the contours and organizes them into a two-level hierarchy. At the top level, there are external boundaries of the components. \n",
    "    # At the second level, there are boundaries of the holes. If there is another contour inside a hole of a connected component, it is still put at the top level.\n",
    "    cnts = cv2.findContours(\n",
    "        canny, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "\n",
    "    for c in range(len(cnts)):\n",
    "        cnt_len = cv2.arcLength(cnts[c], True)\n",
    "\n",
    "        # Small contours will appear in green; can be commented and code will still work\n",
    "        if(cnt_len <= 70):\n",
    "            cv2.drawContours(img, [cnts[c]], 0, (0, 255, 0), 3)\n",
    "            continue \n",
    "\n",
    "        # Recognize most approximate CLOSED Poly-shape to contour to draw\n",
    "        approx_poly = cv2.approxPolyDP(\n",
    "            cnts[c], 0.03 * cv2.arcLength(cnts[c], True), True)\n",
    "\n",
    "        # Rectangle Recognition through Distance and Angle Analysis\n",
    "        ravel = approx_poly.ravel()\n",
    "        n_sides = len(approx_poly)\n",
    "        angle_cdt = False \n",
    "\n",
    "        # NÂº of Sides is 4 = Square/Rectangle\n",
    "        if(n_sides == 4): \n",
    "            angle_cdt = True \n",
    "\n",
    "            cnt_img = np.zeros((len(img), len(img[0])), np.uint8)\n",
    "            angles = calcAngles(cv2.drawContours(cnt_img, [approx_poly], -1, (255), 1), ravel)\n",
    "\n",
    "            for a in angles:\n",
    "                if(abs(a - 90) > 10):\n",
    "                    angle_cdt=False\n",
    "                    break\n",
    "            \n",
    "        if(angle_cdt):\n",
    "            img = cv2.drawContours(img, [approx_poly], -1, (0, 230, 255), 3)\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"result\", img)\n",
    "\n",
    "\n",
    "\n",
    "    # TEST contours\n",
    "    cv2.waitKey()\n",
    "\n",
    "    # Destroys all of the HighGUI windows.\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIFT & FLANN & RANSAC\n",
    "def featureDetectionAndMatching(img, train_list):\n",
    "\n",
    "    MIN_MATCH_COUNT = 5\n",
    "    count = 0\n",
    "    for img_path in train_list:\n",
    "\n",
    "        img_train = cv2.imread(img_path)\n",
    "\n",
    "        # Initiate SIFT detector\n",
    "        sift = cv2.SIFT_create()\n",
    "\n",
    "        \n",
    "        # find the keypoints & compute descriptors\n",
    "        kp, des = sift.detectAndCompute(img_train, None)\n",
    "        kp2, des2 = sift.detectAndCompute(img, None)\n",
    "\n",
    "\n",
    "        # create FLANN feature matcher\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = flann.knnMatch(des, des2, k=2)\n",
    "\n",
    "        # store all the good matches as per Lowe's ratio test.\n",
    "        good = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.7*n.distance:\n",
    "                good.append(m)\n",
    "\n",
    "        if len(good) >= MIN_MATCH_COUNT:\n",
    "            src_pts = np.float32([kp[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "            dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            matchesMask = mask.ravel().tolist()\n",
    "            h, w = img_train.shape[:2]\n",
    "            pts = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]\n",
    "                            ).reshape(-1, 1, 2)\n",
    "            if not M is None:\n",
    "                dst = cv2.perspectiveTransform(pts, M)\n",
    "                img = cv2.polylines(img, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)\n",
    "            else: \n",
    "                print(\"No homography found...\")\n",
    "                continue\n",
    "        else:\n",
    "            print(\"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT))\n",
    "            matchesMask = None\n",
    "            continue\n",
    "\n",
    "\n",
    "        # draw match if matched, keypoints if not\n",
    "        draw_params = dict(matchColor=(0, 255, 0),  # draw matches in green color\n",
    "                        singlePointColor=None,\n",
    "                        matchesMask=matchesMask,  # draw only inliers\n",
    "                        flags=2)\n",
    "\n",
    "\n",
    "        img_result = cv2.drawMatches(img_train, kp, img, kp2, good, None, **draw_params)\n",
    "\n",
    "        img_result = cv2.cvtColor(img_result, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 15))\n",
    "        ax.imshow(img_result, 'gray'), plt.show()\n",
    "\n",
    "        count+=1\n",
    "\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough matches are found - 4/5\n",
      "Not enough matches are found - 4/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 4/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 1/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n",
      "Not enough matches are found - 0/5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(os.path.join(image_path, 'road459.png'))\n",
    "\n",
    "# TODO: meter mais imagens de limite de velocidade recomendado\n",
    "train_set = ['./train-img/crossing.png', './train-img/crossing_1.png',\n",
    "             './train-img/passage_priority.png', './train-img/dead_end.png', './train-img/roads_limits.png', './train-img/via_reservada.png', './train-img/via_reservada_end.png', './train-img/info.png', './train-img/information-lane-end.png', './train-img/information-lane-start.png', './train-img/information-lanes.png', './train-img/information-motorway_end.png', './train-img/information-motorway.png', './train-img/information-one-way-traffic_end.png', './train-img/information-one-way-traffic.png', './train-img/information-parking.png', './train-img/information-speed-recommended_end.png', './train-img/information-speed-recommended.png', './train-img/information-tunnel_end.png', './train-img/information-tunnel.png', './train-img/information-two-way-traffic.png']\n",
    "\n",
    "img_clean = removeShadows(contrastAdjust(img))\n",
    "# featureRecognition(img_clean)\n",
    "\n",
    "# TODO: retornar o numero de matches que encontra (melhoria)\n",
    "featureDetectionAndMatching(img_clean, train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images to check with blue squares\n",
    "\n",
    "- 122 \n",
    "- 123 - needs adjustments\n",
    "- 124\n",
    "- 125 - not working\n",
    "- 126 - not working \n",
    "- 127 - not working\n",
    "- 128 - not working\n",
    "- 129 - not working \n",
    "- 130 - not working \n",
    "- 131 - not working\n",
    "- 132\n",
    "- 133??? (cleaning + sementation)\n",
    "- 134 - needs adjustments (segmentation)\n",
    "- 135 - needs adjustments (segmentation)\n",
    "- 136 - not working\n",
    "- 137 \n",
    "- 138 - not working\n",
    "- 139 - not working (segmentation)\n",
    "- 140 - not working (angles)\n",
    "- 141 - not working (segmentation)\n",
    "- 142 - not working (segmentation)\n",
    "- 143??? \n",
    "- 144 \n",
    "- 145??? angles?\n",
    "- 146 \n",
    "- 147 - same as 145\n",
    "- 148 - needs adjustments (segmentation + contours)\n",
    "- 149 - segmentation\n",
    "- 150 - not working\n",
    "- 151?\n",
    "- 152\n",
    "- 153 - needs adjustments (contours)\n",
    "- 154 - too far\n",
    "- 155 \n",
    "- 156 - contours\n",
    "- 157 - contours\n",
    "- 158 - segmentation\n",
    "- 159 - surprisingly works\n",
    "- 160 - segmentation\n",
    "- 161 - segmentation\n",
    "- 163\n",
    "- 164 - cut in the middle (extra)\n",
    "- 165 - segmentation\n",
    "- 166\n",
    "- 167 - segmentation - dark\n",
    "- 168??\n",
    "- 170 - segmentation + contours\n",
    "- 171 - awful image\n",
    "- 172 - awful image\n",
    "- 174 - segmentation\n",
    "- 175 - segmentation + contours\n",
    "- 176 - segmentation\n",
    "- 178\n",
    "- 180 - segmentation\n",
    "- 183 - segmentation\n",
    "- 184 - recognizes shape when it shouldn't\n",
    "- 186 - segmentation\n",
    "- 187 - segmentation\n",
    "- 188 - contouring\n",
    "- 189 - segmentation \n",
    "- 190 - segmentation\n",
    "- 191 - cut sign (extra)\n",
    "- 193 - contouring\n",
    "- 194 - segmentation\n",
    "- 200 - segmentation + contouring\n",
    "- 204 - segmentation\n",
    "- 205 - segmentation\n",
    "- 206 - contouring\n",
    "- 207 - awful image\n",
    "- 224 - awful image\n",
    "- 226 - contouring\n",
    "- 237 - awful image\n",
    "- 238 - awful image\n",
    "- 239 - awful image\n",
    "- 253 - segmentation\n",
    "- 254 - segmentation\n",
    "- 255 - segmentation\n",
    "- 256 - segmentation\n",
    "- 257 - awful image\n",
    "- 258 - awful image\n",
    "- 259 - segmentation\n",
    "- 260 - segmentation\n",
    "\n",
    "\n",
    "- 294 - not working\n",
    "- 299 - not working\n",
    "- 309\n",
    "- 311\n",
    "- 318"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Consider **checking binarization**, image segmentation with threshholding and feature recognition classic techniques using query images to compare features"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b8aa58cbdd3fa7ee5838f11bebf8643e92d1255fc2cdda3e6cf742529abce79"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
